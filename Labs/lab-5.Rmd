---
title: "Lab 5 - Introduction to Spark with R"
date: "Due: October 19, 2018"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```


# Getting Started

For this labs, we will work with **Spark** to manipulate and to analyze the `nycflight13` containing information about flights departing from NYC in 2013.

While Spark is typically deployed on a cluster of 10s/100s machines to perform the analysis of a very big data, the initial cost of buying/renting/running a cluster and technical expertise requirement to deploy and to administrate such system are very high. You are asked to install a standalone Spark on your local machine to work on this lab.

Spark requires the Java JDK installed on your machine. While the latest version, Java 10, is currently available, Spark has only been tested with Java 8. You can go to [the link to download](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) and install this version.

Once you finish installing the Java JDK, Spark installation can be done in R/RStudio via the `sparklyr` package

```{r, message=FALSE}
# Uncomment the following to install the package. You only need to do it once.
# install.packages("sparklyr")
library(sparklyr)

spark_install(version = "2.3.1")
```

Once you finish the installation, you can connect to your local Spark:

```{r}
sc <- spark_connect(master = "local", version = "2.3.1")
```

Once you finish with Spark, you need to disconnect from Spark and release the reserved memory space Spark uses. Remember to run the following when you are done (but not now):

```{r}
spark_disconnect(sc)
```

The flight dataset is stored in the `nycflights13` package. We will install and load all remaining required packages

```{r}
# Uncomment the following to install the package. You only need to do it once.
# install.packages("nycflights13")
library(dplyr)
library(nycflights13)
library(ggplot2)
```

With the connection to the Spark above, we can copy the flights data using the `copy_to` function. *Caveat:* The flight data in `nycflights13` is convenient for `dplyr` demonstrations because it is small, but in practice large data should rarely be copied directly from R objects.

```{r}
nycflights <- copy_to(sc, flights, "flights")
airlines <- copy_to(sc, airlines, "airlines")
src_tbls(sc)
```

# `dplyr` Verbs

Verbs are dplyr commands for manipulating data. When connected to a Spark DataFrame, `dplyr` translates the commands into Spark SQL statements. Remote data sources use exactly the same five verbs as local data sources. Here are the five verbs with their corresponding SQL commands:

- `select ~ SELECT` : pick columns by name
- `filter ~ WHERE` : keep rows matching criteria
- `arrange ~ ORDER BY` : reorder rows
- `summarise ~ aggregators: sum, min, sd, etc.` : reduce variables to values
- `mutate ~ operators: +, *, log, etc.` : add new variables
- `group_by ~ GROUP BY`: group rows according to a variable's values

The syntax of these verbs is 

```
<DPLYR VERB> ( <OBJECT>, <ARGUMENT OF THE VERB> )
```

Examples:
```{r}
select(nycflights, year:day, arr_delay, dep_delay)
filter(nycflights, dep_delay > 1000)
arrange(nycflights, desc(dep_delay))
mutate(nycflights, speed = distance / air_time * 60)
```

**Question 1:** Use the `dplyr` verbs to find the answer of the following questions:

a. Is there any flights departing between midnight and 4 AM?
b. Which flights were most delayed?
c. Which flights caught up the most time during the flight?
d. Compute speed in mph from time (in minutes) and distance (in miles). Which flight flew the fastest?

**Question 2** Delays are always annoying. However, people have different level of delay tolerance. What is your tolerance on delays (in minutes)? Let use your number as the threshhold to classify "delayed" flights. For example, I have a high tolerance for delay, so any delay less than half an hour is tolerable. So the "delayed" flights are all more than half an hour delayed flights. Based on this new classification, compute the rate of delayed flights each day/.

# Laziness

When working with databases, dplyr tries to be as lazy as possible:

- It never pulls data into R unless you explicitly ask for it.
- It delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step.

For example:
```{r}
c1 <- filter(nycflights, day == 17, month == 5, carrier %in% c('UA', 'WN', 'AA', 'DL'))
c2 <- select(c1, year, month, day, carrier, dep_delay, air_time, distance)
c3 <- arrange(c2, year, month, day, carrier)
c4 <- mutate(c3, air_time_hours = air_time / 60)
```

This sequence of operations never actually touches the database. It's not until you ask for the data (e.g. by printing c4) that dplyr requests the results from the database.

```{r}
c4
```

The `sparklyr` and `dplyr` packages provide the working environment that is almost the same as our usual R enviroment. However, the main difference is that Spark-related objects do not store data like R objects. They are just the address to the data located in Spark. For example, the size of these following objects are:

```{r}
object.size(flights)
object.size(nycflights)
```

You can copy data from Spark into R's memory by using collect().

```{r}
carrierhours <- collect(c4)
```

`collect()` executes the Spark query and returns the results to R for further analysis and visualization.

```{r}
with(carrierhours, pairwise.t.test(air_time, carrier))
```

```{r}
ggplot(carrierhours, aes(carrier, air_time_hours)) + geom_boxplot()
```

# Piping

You can use `magrittr` (automatically loaded with `dplyr`) pipes to write cleaner syntax. Using the same example from above, you can write a much cleaner version like this:

```{r}
c4 <- nycflights %>%
  filter(month == 5, day == 17, carrier %in% c('UA', 'WN', 'AA', 'DL')) %>%
  select(carrier, dep_delay, air_time, distance) %>%
  arrange(carrier) %>%
  mutate(air_time_hours = air_time / 60)
```

Mentally, read `%>%` as "then". For example: in the above sequence, we can read out loud as: "Take the nycflight data, **then** find flights on 5/17/2013 from major airlines,  **then** keep only the carrier, departure delay, flying time and distance, **then** reorder by carrier, **then** compute the flying time in hours."

*Note:* The pipes use the previous results as the first argument of the current verb. If you wish to use the previous results as other argument, use the dot("."). For example:

```{r}
# Don't run this
f(x, y)
y %>% f(x, .)
```

**Question 3:** Use pipes to answer the following questions:

a. Which destinations have the highest average delays?
b. Which flights (i.e., carrier + flight) happen everyday? Where do they fly to?

# Performing Joins

It's rare that a data analysis involves only a single table of data. In practice, you'll normally have many tables that contribute to an analysis, and you need flexible tools to combine them. In dplyr, there are three families of verbs that work with two tables at a time:

- Mutating joins, which add new variables to one table from matching rows in another.
- Filtering joins, which filter observations from one table based on whether or not they match an observation in the other table.
- Set operations, which combine the observations in the data sets as if they were set elements.

All two-table verbs work similarly. The first two arguments are `x` and `y`, and provide the tables to combine. The output is always a new table with the same type as `x`.

The following statements are equivalent:

```{r}
nycflights %>% left_join(airlines)
nycflights %>% left_join(airlines, by = "carrier")
nycflights %>% left_join(airlines, by = c("carrier", "carrier"))
```

In addition to `left_join`, `dplyr` provides `inner_join`, `right_join`, `full_join`. In all the joins, the common named columns will be used as keys for matching, unless specifying by the `by` argument. To join by different variables on `x` and `y` use a named vector. For example, `by = c("a" = "b")` will match `x.a` to `y.b`.

**Question 4:** Weather is typical a main cause of flight delays. In this quesion, you are asked to explore which weather conditions are associated with delays leaving New York City. Perform the following tasks:

a. Compute the average delay per hour and remove any cases when the number of flights are less than 10.
b. The package `nycflights` also has the data set `weather` containing hourly meterological data. Copy the data to Spark.
c. Join the two data sets in Spark
d. Use graphics to explore the possible associations.
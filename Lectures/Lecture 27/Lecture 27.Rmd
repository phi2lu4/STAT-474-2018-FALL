---
title: "STAT 474 &ndash; Techniques for Large Data Sets"
subtitle: "Fall 2018"
date: "November 7, 2018"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(eval=FALSE)
library(stringr)
library(sentimentr)
```

# Sentiment Analysis - Review

- Sentiments means feelings, emotions, attitudes, subjective impressions, but not facts.

- Using NLP (natural language process), statistics, or machine learning methods to extract, identify, or otherwise characterize the sentiment content of a text unit

--

- Keyword methods
  
  - Use lists of keywords with sentiment annotation
  
  - Sentiment of a text segment is the total sentiment "score" of all available tokens
  
  - Different sentiment lists give slightly different, but similar results.

--

- **What should we aware when performing sentiment analysis?**

---
# Feature generation for other purposes

- How to quantify what a document is about?

- With a collection of documents, how should we divide into natural groups so that we can understand them separately?

- With a bunch of emails, how to distinguish the regular versus the spam ones?

--

.center[ How to extract/generate features/variables from a collection of documents? ]

--

- Basic: Bag of words, TF-IDF

- Advanced: word embedding (Word2Vec) - words or phrases from the vocabulary are mapped to vectors of real numbers.

---
# Bag of words

- Use the occurrence (or frequency) of each word to represent a document.

- Example: document = "John likes to watch movies. Mary likes movies too."

  - Break up words: "John", "likes", "to", "watch", "movies", "Mary", "likes", "movies", "too"
  
  - Bag of words: `{"John":1, "likes":2, "to":1, "watch":1, "movies":2 ,"Mary":1, "too":1}`

--

- Can be achieved by tokenizing and count distinct words.

- When working with more than one document (e.g., with a corpus), we usually use the document-term matrix.

---
# Word and Document Frequency: TF-IDF

- Issues with bag of words:

  - Longer documents have more words
  
  - Stop words are the most common.
  
--

- **inverse document frequency** (IDF): decreases the weight for commonly used words and increases the weight for words that are not used very much

- TF-IDF = term frequency - inverse document frequency. Computed as the product of term frequency and quantity for IDF.

$$\mbox{IDF (term)} = \log\left(\frac{n_{\mbox{documents}}}{n_{\mbox{documents containing term}}}\right)$$

- The statistic TF-IDF is intended to measure how important a word is to a document in a collection (or corpus) of documents.

- *Toy example:* d1 = "this a is a sample", d2 = "example: this another example is another example"

---
# Variants of TF-IDF

- The term "frequency" in the simplest choice can be the *raw count*

  - The count grows as the length of the document increases.
  
--

- We can adjust the term frequency as the proportion of the given term, which is the ratio between the *raw count* and the total number of words.

  - Most commonly used, but the ratio tends to be very small.
  
--

- The middle ground: *raw count* on the logarithmical scale.

$$ \mbox{TF} = 1 + \log(\mbox{raw count}) $$

---
# Justification of IDF

- The IDF has the connection with the Zipf's law.

  - Named after the the linguist George Zipf
  
  - Given some corpus, the frequency of any word is inversely proportional to its rank in the frequency table
  
--

- Rare words are more informative than common words. 

  - High weight for rare words and low weight for common words
  
  - Candidate is the document frequency or proportion of containing document.
  
  - Use the log scale to "dampen" the effect of IDF.
  
--

- IDF presents an attempt to put words in the probability framework.

---
# The Vector Space Models

- Also known as **Term Vector Model**, representing text documents as numeric vectors of specific terms. Both bag of words and TF-IDF are vector space models.

--

- A vector space can be denoted as

$$VS = \{W_1, W_2, \ldots, W_n\}$$
where there are $n$ distinct words across all documents.

- A document $D$ in the collection can be represented in this space as

$$D = \{ w_{D1}, w_{D2}, \ldots, w_{Dn}\}$$
where $w_{Di}$ denoted the weight for word $i$ in the document $D$.

--

- Binary incidence: $w_{Di}$ is either 0 or 1, corresponding to the word's presence in the document

--

- Bag of words: $w_{Di}$ is the occurrence of the corresponding word (i.e., *raw count*) in the document $D$.

--

- TF_IDF: use weight tf-idf as the value for $w_{Di}$
---
# The Vector Space Models

- Each document is now presented as a real-valued vector of weights

- The total number of distinct words across all documents is the dimension of the vector space

- Terms are axes of the space

- Documents are **points** or **vectors** in this space

- Very high-dimensional: tens of millions of dimensions when you apply this to web search engines.

- Each vector is very space, as most entries are zero.

- Can be extended by defining terms as phrases (instead of single words)

---
# Jane Austen example in R
  
- Examine the distribution of word counts.

- Verify the Zipf's law

- Using TF-IDF to explore six Jane Austen's novels


---
title: "Example of text analysis"
output: html_notebook
---

In this example, we will revisit the text analysis of Jane Austen's six classic novels and apply some sentiment analysis techniques. Let's get started with loading necessary libraries and installing a new package `wordcloud`, which allows us to create a word cloud graph.

```{r}
library(tidyverse)
library(tidytext)
library(janeaustenr)
```


In the previous demo, we have tidied the work of Jane Austen.

```{r}
original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()
tidy_books <- original_books %>% 
  unnest_tokens(word, text) %>% 
  mutate(word = str_extract(word, "[a-z']+"))
```


The `tidytext` package contains four sentiment lexicons in the sentiments dataset, three general-purpose ones and one suitable for financial text (e.g., where "share" is not necessarily positive and "liability" not necessarily negative). 

The three general-purpose lexicons are

- `AFINN` from [Finn Arup Nielsen](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010),
- `bing` from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and
- `nrc` from [Saif Mohammad and Peter Turney](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm).

All three of these lexicons are based on unigrams, i.e., single words and can be accessed via the function `get_sentiments()`.

```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```

**Question:** Explore the three lexicons and describe their main differences.

With data in a tidy text format, sentiment analysis can be done as an inner join. Let's look at the words with a joy score from the NRC lexicon and find the most common joy words in *Emma*.

```{r}
nrc_joy <- get_sentiments("nrc") %>% filter(sentiment == "joy")

tidy_books %>% filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

We see mostly positive, happy words about hope, friendship, and love here. 

**Question 1:** According to the `bing` lexicon, which word is the most common positive, which one is the most common neggative word?

```{r}
# Your code goes here


```


Furthermore, we can also examine how sentiment changes throughout each novel. First, We find a sentiment score for each word using the Bing lexicon. Then, we count up how many positive and negative words there are in defined sections of each book. 

Note that small sections of text may not have enough words in them to get a good estimate of sentiment while really large sections can wash out narrative structure. For these books, using 80 lines works well, but this can vary depending on individual texts, how long the lines were to start with, etc.

```{r}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  mutate(index = linenumber %/% 80 ) %>%
  count(book, index, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

Now we can plot these sentiment scores across the plot trajectory of each novel. Notice that we are plotting against the index on the x-axis that keeps track of narrative time in sections of text.

```{r}
ggplot(jane_austen_sentiment) +
  geom_col(aes(index, sentiment, fill = book), show.legend = FALSE) +
  facet_wrap( ~ book, ncol = 2, scales = "free_x")
```


**Question 2:** With several options for sentiment lexicons, you might want some more information on which one is appropriate for your purposes. Use all three sentiment lexicons and examine how the sentiment changes across the narrative arc of Pride and Prejudice. 

```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

# Your codes goes here

```

# Better sentiment analysis with `sentimentr`

We will start this section with the installation of the paclage `sentimentr`

```{r}
# install.packages("sentimentr")
library(sentimentr)
```



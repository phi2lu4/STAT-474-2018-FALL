---
title: "Example of text analysis"
output: html_notebook
---

In this example, we will revisit the text analysis of Jane Austen's six classic novels and then compare her works to H.G. Wells' works. Let's get started with loading necessary libraries and installing a new package `wordcloud`, which allows us to create a word cloud graph.

```{r}
# instrall.packages("wordcloud")
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(gutenbergr)
library(wordcloud)
```

# Revisit Jane Austen's works

In the previous demo, we have tidied the work of Jane Austen.

```{r}
original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()
tidy_books <- original_books %>% unnest_tokens(word, text)
```

Similar to R Markdown, some of words are with underscores around them to indicate emphasis (like italics).

```{r}
tidy_books %>% arrange(word)
```

**Question:** Remove all the word formating in the column `word`

```{r}
# Replace ... with your codes.
tidy_books <- ...
```


A typical step in text analysis after tokenization is to remove the stop words. Instead of performing the task, let's explore the effect of stop words in the analysis.

```{r}
tidy_books_with_stop_words <- tidy_books
tidy_books <- tidy_books %>% anti_join(stop_words)
```

The word counts and frequencies are computed 

```{r}
freq_table <- tidy_books %>% count(word, sort = TRUE) %>% 
  mutate(proportion = n/sum(n))
freq_table
```

**Question:** How does the most common words table look like if we keep all the stop words?

```{r}
# Your codes goes here

```

Once we computed the frequency table, we can create various visualizations of the most common words

```{r}
freq_table %>% filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() + geom_col(aes(x = word, y = n)) + coord_flip()
```

Below is a word clound, which use size and/or color of text to present their frequency. *Note:* the `wordcloud()` function uses frequencies, which must be computed before call the function. A word cloud usually displays 25-100 words.

```{r}
with(freq_table, wordcloud(word, n, max.words = 75))
```

**Question:** How does the word cloud look like if we include the stop word?

# Jane Austen versus H.G. Wells

Let's look at some science fiction and fantasy novels by H.G. Wells, who lived in the late 19th and early 20th centuries. Let's get *The Time Machine*, *The War of the Worlds*, *The Invisible Man*, and *The Island of Doctor Moreau*. We can access these works using `gutenberg_download()` and the Project Gutenberg ID numbers for each novel.

```{r}
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
```

**Question:** Let tidy and clean up the text from HG Wells' novels.

```{r}
# Replace ... with codes to answer the above question.
tidied_hgwells <- ... 
```

Let's calculate the frequency for each word and bind the two data set together.

```{r}
frequency <- bind_rows(mutate(tidy_books, author = "Jane Austen"),
                       mutate(tidied_hgwells, author = "H.G. Wells")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n/sum(n)) %>%
  select(-n) %>%
  spread(author, proportion)
```

We finish with a scatterplot of the frequencies

```{r}
ggplot(frequency, aes(x = `Jane Austen`, y = `H.G. Wells`)) +
  geom_jitter(alpha = 0.1, width = .3, height = .3) +
  geom_text(aes(label = word), check_overlap = TRUE) +
  geom_abline(lty = 2) +
  scale_x_log10() + scale_y_log10()
```

# Simple sentiment analysis 

Sentiment analysis is one of the most obvious things we can do with unlabelled text data (with no score or no rating) to extract some insights out of it. One of the most primative sentiment analysis is to perform a simple dictionary lookup and calculate a final composite score based on the number of occurences of positive and negative words. In this section, we will explore this approach with Jane Austen's works.

The `tidytext` package contains four sentiment lexicons in the sentiments dataset, three general-purpose ones and one suitable for financial text (e.g., where "share" is not necessarily positive and "liability" not necessarily negative). 

The three general-purpose lexicons are

- `AFINN` from [Finn Arup Nielsen](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010),
- `bing` from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and
- `nrc` from [Saif Mohammad and Peter Turney](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm).

All three of these lexicons are based on unigrams, i.e., single words and can be accessed via the function `get_sentiments()`.

```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```

**Question:** Explore the three lexicons and describe their main differences.

With data in a tidy text format, sentiment analysis can be done as an inner join. Let's look at the words with a joy score from the NRC lexicon and find the most common joy words in *Emma*.

```{r}
nrc_joy <- get_sentiments("nrc") %>% filter(sentiment == "joy")

tidy_books %>% filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

We see mostly positive, happy words about hope, friendship, and love here. Furthermore, we can also examine how sentiment changes throughout each novel. First, We find a sentiment score for each word using the Bing lexicon. Then, we count up how many positive and negative words there are in defined sections of each book. 

Note that small sections of text may not have enough words in them to get a good estimate of sentiment while really large sections can wash out narrative structure. For these books, using 80 lines works well, but this can vary depending on individual texts, how long the lines were to start with, etc.

```{r}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  mutate(index = linenumber %/% 80 ) %>%
  count(book, index, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

Now we can plot these sentiment scores across the plot trajectory of each novel. Notice that we are plotting against the index on the x-axis that keeps track of narrative time in sections of text.

```{r}
ggplot(jane_austen_sentiment) +
  geom_col(aes(index, sentiment, fill = book), show.legend = FALSE) +
  facet_wrap( ~ book, ncol = 2, scales = "free_x")
```


**Question:** With several options for sentiment lexicons, you might want some more information on which one is appropriate for your purposes. Use all three sentiment lexicons and examine how the sentiment changes across the narrative arc of Pride and Prejudice. 

```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

# Your codes goes here

```


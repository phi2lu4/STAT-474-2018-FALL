<!DOCTYPE html>
<html>
  <head>
    <title>STAT 474 – Techniques for Large Data Sets</title>
    <meta charset="utf-8">
    <meta name="date" content="2018-10-10" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# STAT 474 – Techniques for Large Data Sets
## Fall 2018
### October 10, 2018

---




# Introduction to Apache Spark


- A new, faster, and more advanced engine for Big Data Analytics

  - predicted to overthrow Hadoop soon
  
  - New adopters tend to opt for Spark in their daily data processing activities.

--

- An attractive alternative to Hadoop
  
  - Reduce the processing time by up to 100x when run in memory
    
  - Can run as a standalone application or be deployed on top of Hadoop 
  
  - Supports multiple languages: Java, Python, Scala and R.

--

- Spark is a cluster computing system and not a data storage system.

  - Need an external source of data storage (local host filesystem, HDFS, NoSQL database)
  
---
# Spark versus MapReduce

- Traditional Approach: MapReduce jobs for complex jobs, interactive query, and online event-hub processing involves lots of (slow) disk I/O

&lt;img src="images/vs-hadoop-1.png" width="75%" style="display: block; margin: auto;" /&gt;

--

- Solution: Keep more data in-memory with a new distributed execution engine

&lt;img src="images/vs-hadoop-2.png" width="75%" style="display: block; margin: auto;" /&gt;

---
# SPARK - The Analytics Operating System
### "Enabling New Classes of Intelligent Applications Embedded with Analytics"

.pull-left[
- Spark unifies data, enabling real-time insights

- Spark processes and analyzes data from any data source

- Spark is complementary to Hadoop, but faster with in-memory performance

- Build models quickly. Iterate faster. Apply intelligence

]

.pull-right[
![](images/spark.png)
]
---
# Spark Application Architecture

- A Spark application is initiated from a driver program

&lt;img src="images/architecture.png" width="75%" style="display: block; margin: auto;" /&gt;

--

- Spark execution modes: Standalone with built-in cluster manager (FIFO scheduler), MESOS or YARN as the cluster manager
---
# Spark Application Architecture

&lt;img src="images/spark.jpg" width="50%" style="display: block; margin: auto;" /&gt;

**Spark SQL**

- Provide for relational queries expressed in SQL, HiveQL and Scala

- Seamlessly mix SQL queries with Spark programs 

- DataFrame/Dataset provide a single interface for efficiently working with structured data including Apache Hive, Parquet and JSON files

- Standard connectivity through JDBC/ODBC (think of DBI package in R)

---
# Spark Streaming

- Started in 2012 with first stable release in Spark 0.9.0

--

- Discretized Stream (DStream) programming abstraction

  - Represented as a sequence of RDDs (micro-batches)
  
  - RDD: set of records for a specific time interval
  
  - Supports Scala, Java, and Python (with limitations)

--

- Fundamental architecture: batch processing of datasets

&lt;img src="images/streaming.png" width="75%" style="display: block; margin: auto;" /&gt;

---
# Spark Machine Learning

- Spark ML for machine learning library

--

- Provides common algorithm and utilities

  - Classification
  
  - Regression
  
  - Clustering
  
  - Collaborative filtering
  
  - Dimensionality reduction

--

- Basically spark ML provides you with a toolset to create "pipelines" of different machine learning related transformations on your data. 


---
# Spark ML

| Classification/Regression | Clustering | Feature Extractors |
|---------------------------|------------|--------------------|
| Logistic Regression | K-means | TF-IDF |
| Decision tree classifier | Gaussian mixture | Word2Vec
| Random forest | Latent Dirichlet Allocation | CountVectorizer |
| Gradient booted tree | Bisecting k-means | **Feature Transformers** |
| Mutilayer perceptron classifier | **Collaborative Filtering** | Tokenizer |
| One-vs-rest classifier | Alternating least squares | StopWordRemover |
| Linear regression | **Feature Selectors** | N-gram |
| Generalized linear reg. | VectorSlicer | Binarizer |
| Naive Bayes | RFormula | PCA |
| .... | .... | .... |

---
# Property Graphs


&lt;img src="images/graphx-2.png" width="50%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="images/graphx-1.png" width="50%" style="display: block; margin: auto;" /&gt;

---
# Spark GraphX


- Flexible Graphing

  - GraphX unifies ETL, exploratory analysis, and iterative graph computation

  - You can view the same data as both graphs and  collections, transform and join graphs with RDDs efficiently, and write custom iterative graph algorithms with the API

--

- GraphX is Apache Spark's API for graph and graph-parallel computation

  - [graphframes.github.io/](http://graphframes.github.io/)

  - [spark.apache.org/graphx/](https://spark.apache.org/graphx/)

- GraphX comes with a variety of graph algorithms

  - [ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html
](http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html
)

---
# Sparklyr: Using Spark with R

- **sparklyr** provides bindings to Spark's distributed *machine learning* library. 

- **sparklyr** allows you to access the machine learning routines provided by the `spark.ml` package. 

- Together with sparklyr's `dplyr` interface, you can easily create and tune machine learning workflows on Spark, orchestrated entirely within R. 

- Sparklyr provides three families of functions that you can use with Spark machine learning:

  - Machine learning algorithms for analyzing data (`ml_*`)
  
  - Feature transformers for manipulating individual features (`ft_*`)
  
  - Functions for manipulating Spark DataFrames (`sdf_*`)

---
# Sparklyr and dplyr as an R Interface for Spark ML

- Interactively manipulate Spark data using both `dplyr` and SQL (via `DBI`).

- Filter and aggregate Spark datasets then bring them into R for analysis and visualization.

- Orchestrate distributed machine learning from R using either `Spark ML` or `H2O SparkingWater` extension.

- Integrated support for establishing Spark connections and browsing Spark DataFrames within the RStudio IDE.

- IBM has incorporated `sparklyr` into its Data Science Experience as well.

---
# Analytic Workflow with `dplyr`

- Perform SQL queries through the sparklyr dplyr interface,

--

- Use the `sdf_*` and `ft_*` family of functions to generate new columns, or partition your data set,

--

- Choose an appropriate machine learning algorithm from the `ml_*` family of functions to model your data,

--

- Inspect the quality of your model fit, and use it to make predictions with new data.

--

- Collect the results for visualization and further analysis in R.

---
# Spark ML machine learning library
### Can be accessed from `sparklyr` through the `ml_*` set of functions:

| Function | Description |
|----------|-------------|
| `ml_kmeans` | K-Means clustering |
| `ml_linear_regression` | Linear Regression | 
| `ml_logistic_regression` | Logistic Regression |
| `ml_decision_tree` | Decision Trees |
| `ml_random_forest` | Random Forests |
| `ml_gradient_boosted_tree` | Gradient-Boosted Trees |
| `ml_pca` | Principal Component Analysis |
| `ml_naive_bayes` | Naive Bayes Classifier |
| ... | ... |

---
# Manipulating Data with `dplyr`

- `dplyr` is an R package for working with structured data both in and outside of R. dplyr makes data manipulation for R users easy, consistent, and performant. With dplyr as an interface to manipulating Spark DataFrames, you can:

  - Select, filter, and aggregate data
  
  - Use window functions (e.g. for sampling)
  
  - Perform joins on DataFrames
  
  - Collect data from Spark into R

--

- You can read data into Spark DataFrames using the following:

  - `spark_read_csv`	Reads a CSV file and provides a data source compatible with `dplyr`
  
  - `spark_read_json`	Reads a JSON file and provides a data source compatible with `dplyr`

---
# Install Standalone Spark

- Need to install Java JSK before Spark. [Link to download](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)

- In the console of RStudio


```r
library(sparklyr)
*spark_install(version = "2.1.0")
```

--

- Create a Spark session


```r
conf &lt;- spark_config()
conf$`sparklyr.shell.driver-memory` &lt;- "16G"  
conf$spark.memory.fraction &lt;- 0.8 
```

--

- Connect to Spark:


```r
sc &lt;- spark_connect(master = "local", config = conf, version = "2.1.0")
```
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
